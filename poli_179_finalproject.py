# -*- coding: utf-8 -*-
"""POLI-179-finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136uqFSScWXh5dPWOGtXCVEd-7eHbf-fm
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install keras
!pip install natsort
!pip install scikit-learn
!pip install Pillow

!pip install keras.layers

!git clone https://github.com/yilangpeng/transfer-learning-clustering.git
!git clone https://github.com/GKalliatakis/Keras-VGG16-places365.git

import os
import keras
import numpy as np
import pandas as pd
import natsort
import sklearn
from PIL import Image

!apt-get install openjdk-11-jdk-headless -qq > /dev/null

#dataset cleaning

def get_image_filenames(folder):
  image_filenames = []
  for filename in os.listdir(folder):
    if filename.lower().endswith(".jpg"):
      image_filenames.append(filename)
  return image_filenames

def write_filenames_to_txt(image_filenames, output_file):
    with open(output_file, 'w') as file:
        for filename in image_filenames:
            file.write(f"{filename}\n")


folder = "/content/drive/MyDrive/POLI179Project/img_all"

output_file = "/content/drive/MyDrive/POLI179Project/imgname2.txt"

image_filenames = get_image_filenames(folder)

write_filenames_to_txt(image_filenames, output_file)

print(image_filenames)

import os, shutil

def create_path(filepath):
    filepathfolder = os.path.dirname(filepath)
    if not os.path.exists(filepathfolder): os.makedirs(filepathfolder)

def copy_file(filepath1, filepath2):
    filepathfolder2 = os.path.dirname(filepath2)
    if not os.path.exists(filepathfolder2): os.makedirs(filepathfolder2)
    shutil.copy2(filepath1, filepath2)

! cp /content/drive/MyDrive/POLI179Project/transfer-learning-clustering-master/transfer-learning-clustering-master/vgg16_hybrid_places_1365.py /content/vgg16_hybrid_places_1365.py

! cp /content/drive/MyDrive/POLI179Project/transfer-learning-clustering-master/transfer-learning-clustering-master/ypoften.py /content/ypoften.py

! cp /content/drive/MyDrive/POLI179Project/Keras-VGG16-places365-master/Keras-VGG16-places365-master/vgg16_hybrid_places_1365.py /content/vgg16_hybrid_places_1365.py

!pip install keras_applications

#Step 1 Feature Extraction
import os, joblib
import numpy as np
from keras.preprocessing import image
from keras.models import Model
from keras.applications.vgg16 import preprocess_input
import ypoften as of
from keras.layers import Dense, Activation, Dropout


cvmodel = "vgg16"
cvmodel = "vgg16hybrid"

if cvmodel == "vgg16":
    from keras.applications.vgg16 import VGG16
    base_model = VGG16(weights="imagenet", include_top=True)
    print(base_model.summary())
    feature_model = Model(base_model.input, base_model.get_layer('fc1').output)
    img_size = (224, 224)

elif cvmodel == "vgg16hybrid":
    #vgg16_hybrid_places_1365 = open("/content/drive/MyDrive/POLI179Project/transfer-learning-clustering-master/transfer-learning-clustering-master/vgg16_hybrid_places_1365.py"
    from vgg16_hybrid_places_1365 import VGG16_Hybrid_1365
    #vgg16_hybrid_places_13
    base_model = VGG16_Hybrid_1365(weights='places', include_top=True, input_shape=(224, 224, 3))
    print(base_model.summary())
    feature_model = Model(base_model.input, base_model.get_layer('fc2').output)
    img_size = (224, 224)

print(feature_model.summary())

cwd = os.path.join("/content/drive/MyDrive/POLI179Project")
# replace with your own folder path
imgnamefile = os.path.join(cwd,"imgname2.txt")
imgnamefile = open(imgnamefile, "r")
exfolder = os.path.join(cwd,"img exfeature", "")

for j, line in enumerate(imgnamefile.readlines()[:]):
    imgname = line.rstrip('\n\r')
    print(j, imgname)

    imgpath = os.path.join(cwd, 'img_all', imgname)
    img = image.load_img(imgpath, target_size=img_size) # read the image
    image_array = image.img_to_array(img) # convert the image to a numpy array

    image_expand = np.expand_dims(image_array, 0)
    x_train = preprocess_input(image_expand) # normalize the image to 0-1 range
    features_x = feature_model.predict(x_train) # extract features for each image

    featuresavepath = os.path.join(exfolder, cvmodel, imgname+".dat")
    of.create_path(featuresavepath)
    joblib.dump(features_x[0], featuresavepath) # save the extracted features
    print("==SUCCESS=="*20)

print("DONE"*20)

#STep 2 Combine Features
cwd = os.path.join("/content/drive/MyDrive/POLI179Project") # replace with your own folder path
cvmodel = "vgg16hybrid"

imgnamefile = os.path.join(cwd,"imgname2.txt")
imgnamefile = open(imgnamefile, "r")
exfolder = os.path.join(cwd,"img exfeature", "")
features_array = []

for lineindex, line in enumerate(imgnamefile.readlines()[:]):
    imgname = line.rstrip('\r\n')
    ex_feature_path = os.path.join(cwd, 'img exfeature', cvmodel, imgname + ".dat")
    imgexfeatures = joblib.load(ex_feature_path)
    flatten_features = np.ndarray.flatten(imgexfeatures)
    features_array.append(flatten_features)

features_array = np.array(features_array)
features_savepath = os.path.join(cwd, 'img exfeature','features combine', cvmodel + ".dat")
of.create_path(features_savepath)
joblib.dump(features_array,features_savepath)

print("DONE"*20)

#Step 3 PCA
cwd = os.path.join("/content/drive/MyDrive/POLI179Project") # replace with your own folder path
cvmodel = "vgg16hybrid"

imgnamefile = os.path.join(cwd,"imgname2.txt")
imgnamefile = open(imgnamefile, "r")
exfolder = os.path.join(cwd,"img exfeature", "")

features_savepath = os.path.join(cwd,'img exfeature','features combine',cvmodel+'.dat')

features_array = joblib.load(features_savepath)
x = pd.DataFrame(features_array)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# standardizing the features
x = StandardScaler().fit_transform(x)

# PCA
pca = PCA(0.95) #95% of the variance is retained.
pca.fit(x)
x_pca = pca.transform(x)

# save PCA
savefolder = os.path.join(cwd,cwd,'img exfeature','features PCA')
pca_savepath = os.path.join(savefolder, cvmodel+'.dat')
of.create_path(pca_savepath)
joblib.dump(x_pca, pca_savepath)

# save components
components = pca.components_
savepath = os.path.join(savefolder, cvmodel + 'components.txt')
components = pd.DataFrame(components)
components.to_csv(savepath, header=None, index=None, sep='\t', mode='a')

# save variance explained ratio
variance = pca.explained_variance_ratio_
savepath = os.path.join(savefolder, cvmodel + ' variance.txt')
variance = pd.DataFrame(variance)
variance.to_csv(savepath, header=None, index=None, sep='\t', mode='a')

print("DONE"*20)

#Step 4 K means Clustering
cwd = os.path.join("/content/drive/MyDrive/POLI179Project") # replace with your own folder path
cvmodel = "vgg16hybrid"
clmethod = "KMeans"

from sklearn.cluster import KMeans
import joblib
import pandas as pd

features_savepath = os.path.join(cwd,'img exfeature','features PCA',cvmodel+'.dat')
features_array = joblib.load(features_savepath)
X = pd.DataFrame(features_array)
print(X)

nd = 200
X = X.iloc[:,0:nd]

savefolder = cvmodel + ' ' + clmethod + ' PCA' + str(nd)

for K in range(5, 21):
    print('number of cluster', K)
    cl = KMeans(K, random_state=0)
    cl.fit(X)
    labels = cl.labels_

    imgnamefile = os.path.join(cwd,"imgname2.txt")
    df = pd.read_csv(imgnamefile, sep ='\t', header = None)
    df.columns = ['imgname2']
    print(df)

    df['label'] = labels
    filepath = os.path.join(cwd,'img cluster',savefolder,str(K),'label.txt')
    of.create_path(filepath)
    df.to_csv(filepath, index = None, header = None, sep = '\t')

print("DONE"*20)

#Step 5 Copy Image
cwd = os.path.join("/content/drive/MyDrive/POLI179Project") # replace with your own folder path
cvmodel = "vgg16hybrid"
clmethod = "KMeans"

savefolder = "vgg16hybrid KMeans PCA200"

def save_img(random_id, imgname, label, K):
    imgpath1=os.path.join(cwd,'img_all',imgname)
    newimgname = str(random_id)+' '+imgname
    imgpath2 = os.path.join(cwd,'img cluster',savefolder, str(K),str(label),newimgname)
    of.copy_file(imgpath1, imgpath2)

for K in range(5, 21):
    print('number of cluster',K)
    filepath = os.path.join(cwd,'img cluster',savefolder,str(K),'label.txt')
    df = pd.read_csv(filepath, sep ='\t', header = None)
    df.columns = ['imgname',"label"]

    dr = df.sample(frac=1, random_state=42) # random shuffle images
    dr['random_id'] = np.arange(len(dr)) # add random id

    for labelK in range(0, K):
        dc = dr.loc[dr['label'] == labelK] # select images from each cluster
        dc = dc.iloc[:20,:] # select 20 images from each cluster
        dc.apply(lambda row: save_img(row['random_id'],row['imgname'],row['label'], K),axis=1)

    print("DONE"*20)
print("DONE"*20)

#Step 6 Visualize Grid

from PIL import Image, ImageDraw, ImageFont
import os
import glob
from natsort import natsorted
def fill_square(im, tbw):
    size = (tbw, tbw)
    bg = Image.new('RGB', size, "white")  # create a background image
    im.thumbnail(size, Image.LANCZOS)
    w, h = im.size
    bg.paste(im, (int((size[0]-w)/2), int((size[1]-h)/2)))
    return(bg)

cwd = os.path.join("/content/drive/MyDrive/POLI179Project") # replace with your own folder path
cvmodel = "vgg16hybrid"
clmethod = "KMeans"

savefolder = "vgg16hybrid KMeans PCA200"

w_sq = 300 # width/height of image
w_clusterid = 60 # width of text for cluster id
h_imgid = 60 # height of gap
fntcluster = ImageFont.load_default()

for K in range(5, 21):
    ncol = 20 # how many images to show in each row
    nrow = 1
    nimg = ncol * nrow # total number of image in each cluster to show
    large_w = w_sq*ncol + w_clusterid # total width of the entire image
    large_h = (w_sq + h_imgid) * nrow * K # total height of the entire image
    large = Image.new('RGB', (large_w, large_h), "white")

    for label in range(0, K):
        print('-'* 10, K, label)
        imgpathfolder = os.path.join(cwd,'img cluster',savefolder,str(K),str(label),'')
        imgfiles = glob.glob(imgpathfolder + "*.jpg")
        imgfiles = natsorted(imgfiles)[:nimg]

        d = ImageDraw.Draw(large)
        d.text((0, label * nrow * (w_sq + h_imgid) + 15), str(label+1), font=fntcluster, fill=(0, 0, 0))

        for j, imgfile in enumerate(imgfiles):
            jw = j%ncol; jh = j//ncol
            img = Image.open(imgfile)
            im_sq = fill_square(img, w_sq)
            im_sq_x = jw*w_sq + w_clusterid
            im_sq_y = h_imgid + jh* (w_sq + h_imgid) + label * nrow * (w_sq + h_imgid)
            large.paste(im_sq, (im_sq_x, im_sq_y))

    large_savepath = os.path.join(cwd, "img cluster", savefolder, 'grid', str(K)+".png")
    of.create_path(large_savepath)
    large.save(large_savepath)

print("DONE"*20)